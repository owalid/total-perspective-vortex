{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-06 22:48:08.876950: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import entropy\n",
    "from pywt import wavedec\n",
    "from scipy.signal import welch\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from mne_features.univariate import compute_spect_entropy, compute_wavelet_coef_energy\n",
    "# https://mne.tools/mne-features/api.html\n",
    "\n",
    "import mne\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "import glob\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from mne.decoding import CSP, SPoC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from mne.decoding import (\n",
    "    SlidingEstimator,\n",
    "    GeneralizingEstimator,\n",
    "    Scaler,\n",
    "    cross_val_multiscore,\n",
    "    LinearModel,\n",
    "    get_coef,\n",
    "    Vectorizer,\n",
    "    CSP,\n",
    ")\n",
    "import numpy as np\n",
    "from mne.preprocessing import ICA\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from utils import preprocess_data\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Conv1D, MaxPooling1D, Dropout, Flatten, BatchNormalization, Conv2D, DepthwiseConv2D, AveragePooling2D, Activation, SeparableConv2D, SpatialDropout1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import mne\n",
    "from mne.datasets import eegbci\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "import glob\n",
    "import numpy as np\n",
    "from utils import preprocess_data\n",
    "from mne.preprocessing import ICA\n",
    "from sklearn.decomposition import FastICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/ftp/arxiv/papers/1312/1312.2877.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Extracting EDF parameters from /Users/owalid/42/post_intership/total-perspective-vortex/files/S001/S001R11.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19999  =      0.000 ...   124.994 secs...\n",
      "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
      "9\n",
      "Extracting EDF parameters from /Users/owalid/42/post_intership/total-perspective-vortex/files/S001/S001R06.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19999  =      0.000 ...   124.994 secs...\n",
      "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
      "13\n",
      "Extracting EDF parameters from /Users/owalid/42/post_intership/total-perspective-vortex/files/S001/S001R02.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Used Annotations descriptions: ['T0']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "    <tr>\n",
       "        <th>Measurement date</th>\n",
       "        \n",
       "        <td>August 12, 2009  16:15:00 GMT</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Experimenter</th>\n",
       "        \n",
       "        <td>Unknown</td>\n",
       "        \n",
       "    </tr>\n",
       "        <th>Participant</th>\n",
       "        \n",
       "        <td>Unknown</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Digitized points</th>\n",
       "        \n",
       "        <td>Not available</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Good channels</th>\n",
       "        <td>64 EEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Bad channels</th>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>EOG channels</th>\n",
       "        <td>Not available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>ECG channels</th>\n",
       "        <td>Not available</td>\n",
       "    \n",
       "    <tr>\n",
       "        <th>Sampling frequency</th>\n",
       "        <td>160.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Highpass</th>\n",
       "        <td>0.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Lowpass</th>\n",
       "        <td>80.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "</table>"
      ],
      "text/plain": [
       "<Info | 7 non-empty values\n",
       " bads: []\n",
       " ch_names: Fc5., Fc3., Fc1., Fcz., Fc2., Fc4., Fc6., C5.., C3.., C1.., ...\n",
       " chs: 64 EEG\n",
       " custom_ref_applied: False\n",
       " highpass: 0.0 Hz\n",
       " lowpass: 80.0 Hz\n",
       " meas_date: 2009-08-12 16:15:00 UTC\n",
       " nchan: 64\n",
       " projs: []\n",
       " sfreq: 160.0 Hz\n",
       ">"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob('../../files/S001/*.edf')\n",
    "\n",
    "'''\n",
    "https://github.com/mne-tools/mne-python/blob/main/mne/datasets/eegbci/eegbci.py#L110\n",
    "=========  ===================================\n",
    "run        task\n",
    "=========  ===================================\n",
    "1          Baseline, eyes open\n",
    "2          Baseline, eyes closed\n",
    "3, 7, 11   Motor execution: left vs right hand\n",
    "4, 8, 12   Motor imagery: left vs right hand\n",
    "5, 9, 13   Motor execution: hands vs feet\n",
    "6, 10, 14  Motor imagery: hands vs feet\n",
    "=========  ===================================\n",
    "'''\n",
    "raws = []\n",
    "f = [5,9,13]\n",
    "# f = [6,10,14]\n",
    "for i in f:\n",
    "    print(i)\n",
    "    current_file = files[i-1]\n",
    "    r = read_raw_edf(current_file, preload=True, stim_channel='auto')\n",
    "    events, _ = mne.events_from_annotations(r)\n",
    "    if i in [5, 9, 13]:\n",
    "        new_labels_events = {1:'rest', 2:'action_hand', 3:'action_feet'} # action\n",
    "    else:\n",
    "        new_labels_events = {1:'rest', 2:'imagine_hand', 3:'imagine_feet'} # imagine\n",
    "    new_annot = mne.annotations_from_events(events=events, event_desc=new_labels_events, sfreq=r.info['sfreq'], orig_time=r.info['meas_date'])\n",
    "    r.set_annotations(new_annot)\n",
    "    raws.append(r)\n",
    "    \n",
    "raw_obj = concatenate_raws(raws)\n",
    "\n",
    "original_raw = raw_obj.copy()\n",
    "original_raw.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up band-stop filter from 49 - 51 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 49.38\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 49.12 Hz)\n",
      "- Upper passband edge: 50.62 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 50.88 Hz)\n",
      "- Filter length: 1057 samples (6.606 sec)\n",
      "\n",
      "Filtering raw data in 3 contiguous segments\n",
      "Setting up band-pass filter from 8 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 265 samples (1.656 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  64 out of  64 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  64 out of  64 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "    <tr>\n",
       "        <th>Measurement date</th>\n",
       "        \n",
       "        <td>August 12, 2009  16:15:00 GMT</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Experimenter</th>\n",
       "        \n",
       "        <td>Unknown</td>\n",
       "        \n",
       "    </tr>\n",
       "        <th>Participant</th>\n",
       "        \n",
       "        <td>Unknown</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Digitized points</th>\n",
       "        \n",
       "        <td>Not available</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Good channels</th>\n",
       "        <td>64 EEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Bad channels</th>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>EOG channels</th>\n",
       "        <td>Not available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>ECG channels</th>\n",
       "        <td>Not available</td>\n",
       "    \n",
       "    <tr>\n",
       "        <th>Sampling frequency</th>\n",
       "        <td>160.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Highpass</th>\n",
       "        <td>8.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Lowpass</th>\n",
       "        <td>30.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Filenames</th>\n",
       "        <td>S001R11.edf&lt;br&gt;S001R06.edf&lt;br&gt;S001R02.edf</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <th>Duration</th>\n",
       "        <td>00:05:11 (HH:MM:SS)</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<RawEDF | S001R11.edf, 64 x 49760 (311.0 s), ~24.4 MB, data loaded>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notch_freq = 50\n",
    "low_cutoff = 8\n",
    "high_cutoff = 30\n",
    "\n",
    "raw_obj.notch_filter(notch_freq, fir_design='firwin')\n",
    "raw_obj.filter(low_cutoff, high_cutoff, fir_design='firwin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['action_feet', 'action_hand']\n",
      "Filtering raw data in 3 contiguous segments\n",
      "Setting up band-pass filter from 8 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 265 samples (1.656 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  64 out of  64 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 3 contiguous segments\n",
      "Setting up band-pass filter from 8 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 265 samples (1.656 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  64 out of  64 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 3 contiguous segments\n",
      "Setting up low-pass filter at 3 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal lowpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Upper passband edge: 3.00 Hz\n",
      "- Upper transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 4.00 Hz)\n",
      "- Filter length: 265 samples (1.656 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  64 out of  64 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "30 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "30 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "30 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    }
   ],
   "source": [
    "event_id = {'action_hand': 1, 'action_feet': 2}\n",
    "events, event_dict = mne.events_from_annotations(original_raw, event_id=event_id)\n",
    "\n",
    "# Epoch your data for different analyses\n",
    "tmin_erd = -2\n",
    "tmax_erd = 0\n",
    "tmin_ers = 4.1\n",
    "tmax_ers = 5.1\n",
    "tmin_mrcp = -2\n",
    "tmax_mrcp = 0\n",
    "\n",
    "raw_obj_erd = raw_obj.copy()\n",
    "raw_obj_erd.filter(8, 30, fir_design='firwin')\n",
    "raw_obj_ers = raw_obj.copy()\n",
    "raw_obj_ers.filter(8, 30, fir_design='firwin')\n",
    "raw_obj_mrcp = raw_obj.copy()\n",
    "raw_obj_mrcp.filter(None, 3, fir_design='firwin')\n",
    "\n",
    "erd_epochs = mne.Epochs(raw_obj, events, event_id=event_id, tmin=tmin_erd, tmax=tmax_erd, baseline=None)\n",
    "ers_epochs = mne.Epochs(raw_obj, events, event_id=event_id, tmin=tmin_ers, tmax=tmax_ers, baseline=None)\n",
    "mrcp_epochs = mne.Epochs(raw_obj, events, event_id=event_id, tmin=tmin_mrcp, tmax=tmax_mrcp, baseline=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(30, 64, 321)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "erd_epochs.get_data().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_from_epoch_one(epoch_data, ica):\n",
    "    current_data = epoch_data\n",
    "    activation_vector = ica.fit(current_data)\n",
    "    activation_vector = activation_vector.components_\n",
    "    mean_current_data = np.mean(current_data, axis=0)\n",
    "    activation_vector -= mean_current_data\n",
    "    mean_activation_vector = np.mean(activation_vector, axis=1)\n",
    "    std_activation_vector = np.std(activation_vector, axis=1)\n",
    "    power_activation_vector = np.mean(np.abs(activation_vector) ** 2, axis=1)\n",
    "    energy_activation_vector = np.sum(np.abs(activation_vector) ** 2, axis=1)\n",
    "    variance_activation_vector = np.var(activation_vector, axis=1)\n",
    "    rms_activation_vector = np.sqrt(np.mean(activation_vector ** 2, axis=1))\n",
    "\n",
    "    epoch_features = np.hstack((mean_activation_vector, std_activation_vector, energy_activation_vector, power_activation_vector, variance_activation_vector, rms_activation_vector))\n",
    "    return epoch_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 161 original time points ...\n",
      "2 bad epochs dropped\n",
      "Using data from preloaded Raw for 28 events and 161 original time points ...\n",
      "Using data from preloaded Raw for 28 events and 161 original time points ...\n",
      "Using data from preloaded Raw for 28 events and 161 original time points ...\n",
      "Using data from preloaded Raw for 28 events and 161 original time points ...\n",
      "Using data from preloaded Raw for 28 events and 161 original time points ...\n",
      "Using data from preloaded Raw for 28 events and 161 original time points ...\n",
      "Using data from preloaded Raw for 28 events and 161 original time points ...\n",
      "Using data from preloaded Raw for 28 events and 161 original time points ...\n",
      "Using data from preloaded Raw for 28 events and 161 original time points ...\n",
      "Using data from preloaded Raw for 28 events and 161 original time points ...\n",
      "Using data from preloaded Raw for 28 events and 161 original time points ...\n",
      "Using data from preloaded Raw for 28 events and 161 original time points ...\n",
      "Using data from preloaded Raw for 28 events and 161 original time points ...\n",
      "Using data from preloaded Raw for 28 events and 161 original time points ...\n",
      "Using data from preloaded Raw for 28 events and 161 original time points ...\n",
      "Using data from preloaded Raw for 28 events and 161 original time points ...\n",
      "Using data from preloaded Raw for 28 events and 161 original time points ...\n",
      "Using data from preloaded Raw for 28 events and 161 original time points ...\n",
      "Using data from preloaded Raw for 28 events and 161 original time points ...\n",
      "Using data from preloaded Raw for 28 events and 161 original time points ...\n",
      "Using data from preloaded Raw for 28 events and 161 original time points ...\n",
      "Using data from preloaded Raw for 28 events and 161 original time points ...\n",
      "Using data from preloaded Raw for 28 events and 161 original time points ...\n",
      "Using data from preloaded Raw for 28 events and 161 original time points ...\n",
      "Using data from preloaded Raw for 28 events and 161 original time points ...\n",
      "Using data from preloaded Raw for 28 events and 161 original time points ...\n",
      "Using data from preloaded Raw for 28 events and 161 original time points ...\n",
      "Using data from preloaded Raw for 28 events and 161 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "0 bad epochs dropped\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "Using data from preloaded Raw for 30 events and 321 original time points ...\n",
      "(88,)\n",
      "(88, 384)\n"
     ]
    }
   ],
   "source": [
    "n_components = None\n",
    "feature_matrix = []\n",
    "labels = []\n",
    "ica = FastICA(n_components=n_components, random_state=97, max_iter=800)\n",
    "\n",
    "for ii in range(erd_epochs.get_data().shape[0]):\n",
    "    current_data_erd = erd_epochs.get_data()[ii]\n",
    "    features_erd = get_features_from_epoch_one(current_data_erd, ica)\n",
    "    feature_matrix.append(features_erd)\n",
    "\n",
    "yy_erd = erd_epochs.events[:, -1] - 1\n",
    "\n",
    "for ii in range(ers_epochs.get_data().shape[0]):\n",
    "    current_data_ers = ers_epochs.get_data()[ii]\n",
    "    features_ers = get_features_from_epoch_one(current_data_ers, ica)\n",
    "    feature_matrix.append(features_ers)\n",
    "yy_ers = ers_epochs.events[:, -1] - 1\n",
    "\n",
    "for ii in range(mrcp_epochs.get_data().shape[0]):\n",
    "    current_data_mrcp = mrcp_epochs.get_data()[ii]\n",
    "    features_mrcp = get_features_from_epoch_one(current_data_mrcp, ica)\n",
    "    feature_matrix.append(features_mrcp)\n",
    "yy_mrcp = mrcp_epochs.events[:, -1] - 1\n",
    "\n",
    "y = np.hstack((yy_erd, yy_ers, yy_mrcp))\n",
    "print(y.shape)\n",
    "feature_matrix = np.array(feature_matrix)\n",
    "print(feature_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = y.copy()\n",
    "features = feature_matrix.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((88,), (88, 384))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape, feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_matrix, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 38.89%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create and train the Naive Bayes classifier\n",
    "nb_classifier = GaussianNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels on the test set\n",
    "nb_predictions = nb_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
    "print(f'Naive Bayes Accuracy: {nb_accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN Accuracy: 38.89%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Create and train the ANN classifier\n",
    "ann_classifier = MLPClassifier(hidden_layer_sizes=(5), max_iter=5800, activation='relu', random_state=42)\n",
    "ann_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels on the test set\n",
    "ann_predictions = ann_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "ann_accuracy = accuracy_score(y_test, ann_predictions)\n",
    "print(f'ANN Accuracy: {ann_accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: MLP\n",
      "Best Parameters: {'model__hidden_layer_sizes': (200, 100), 'model__max_iter': 5000}\n",
      "Best Cross-Validated Accuracy: 0.54\n",
      "\n",
      "\n",
      "Model: Decision Tree\n",
      "Best Parameters: {'model__max_depth': 5, 'model__min_samples_split': 2}\n",
      "Best Cross-Validated Accuracy: 0.81\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shuffle_split = ShuffleSplit(n_splits=10, test_size=0.3, random_state=42)\n",
    "\n",
    "models = [\n",
    "    # ('Gradient Boosting', GradientBoostingClassifier(), {'model__n_estimators': [50, 100]}),\n",
    "    # ('Linear discriminant analysis', LinearDiscriminantAnalysis(), {'model__solver': ['svd'], 'model__tol': [0.0001, 0.00001]}),   \n",
    "    # ('Random Forest', RandomForestClassifier(), {'model__n_estimators': [50,100,200,300,400]}),\n",
    "    # ('SVM', SVC(), {'model__C': [0.5, 1, 3, 5, 10], 'model__gamma': [3, 4, 5, 10], 'model__degree': [1, 3, 5, 8], 'model__kernel': ['linear', 'rbf']}),\n",
    "    # ('KNN', KNeighborsClassifier(), {'model__n_neighbors': [4,5,6]}),\n",
    "    ('MLP', MLPClassifier(), {'model__hidden_layer_sizes': [(100, 50), (200, 100)],  'model__max_iter': [5000]}),\n",
    "    ('Decision Tree', DecisionTreeClassifier(), {'model__max_depth': [10, 15, 20, 30, 50], 'model__min_samples_split': [2, 10, 15], 'model__max_depth': [2, 3, 5, 10, 15]}),\n",
    "    # ('XGB', XGBClassifier(), {'model__n_estimators': [50, 100, 200], 'model__learning_rate': [0.0001, 0.045, 0.05, 0.001]})\n",
    "]\n",
    "\n",
    "pipelines = []\n",
    "# csp = CSP()\n",
    "for name, model, param_grid in models:\n",
    "    pipeline = Pipeline([\n",
    "        # ('csp', csp),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    # param_grid['csp__n_components'] = [5, 10, 15]\n",
    "    pipelines.append((name, pipeline, param_grid))\n",
    "\n",
    "\n",
    "results = []\n",
    "for name, pipeline, param_grid in pipelines:\n",
    "    grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=shuffle_split, n_jobs=-1)\n",
    "    grid_search.fit(features, labels)\n",
    "    results.append((name, grid_search))\n",
    "\n",
    "\n",
    "res_grid = []\n",
    "for name, grid_search in results:\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best Cross-Validated Accuracy: {grid_search.best_score_:.2f}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
