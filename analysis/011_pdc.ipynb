{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.frontiersin.org/articles/10.3389/fnhum.2016.00235/full\n",
    "\n",
    "https://www.mdpi.com/1424-8220/21/19/6570\n",
    "\n",
    "\n",
    "https://github.com/dokato/connectivipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La Coherence Dirigée Partielle, ou PDC (Partial Directed Coherence en anglais), est une mesure de la connectivité dans le domaine de la neurophysiologie et de l'analyse du cerveau. Elle est utilisée pour étudier comment différentes régions du cerveau interagissent les unes avec les autres.\n",
    "\n",
    "La PDC est particulièrement utile pour comprendre comment l'activité électrique ou électroencéphalographique (EEG) est propagée entre différentes régions du cerveau. Voici quelques points clés pour comprendre la PDC :\n",
    "\n",
    "Connectivité Dirigée : La PDC permet de mesurer la direction de la connectivité entre les régions cérébrales. Autrement dit, elle indique comment l'activité d'une région du cerveau influence l'activité d'une autre région, et vice versa.\n",
    "\n",
    "Fréquences et Bandes de Fréquences : La PDC peut être calculée pour différentes fréquences ou bandes de fréquences du signal EEG. Cela permet de comprendre comment la connectivité entre les régions cérébrales varie en fonction de la fréquence.\n",
    "\n",
    "Utilisation : La PDC est largement utilisée dans la recherche en neurosciences pour étudier des phénomènes tels que la communication cérébrale, la synchronisation entre les régions cérébrales, et les changements dans la connectivité en réponse à des tâches spécifiques ou à des pathologies.\n",
    "\n",
    "Interprétation : Une PDC proche de 1 entre deux régions signifie une forte connectivité dirigée de l'une vers l'autre, tandis qu'une PDC proche de 0 indique une connectivité faible ou nulle.\n",
    "\n",
    "Applications : La PDC est utilisée dans divers domaines, y compris la recherche sur les troubles neurologiques, l'étude de la cognition humaine, la neurologie clinique, et même dans le domaine de l'analyse de l'activité cérébrale lors de tâches spécifiques telles que la réflexion ou la perception.\n",
    "\n",
    "En résumé, la PDC est une mesure importante pour comprendre comment différentes parties du cerveau interagissent et communiquent entre elles. Elle est souvent utilisée dans le cadre de la recherche en neurosciences pour mieux comprendre le fonctionnement du cerveau et les altérations qui peuvent survenir dans diverses conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from mne.datasets import eegbci\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "import glob\n",
    "import numpy as np\n",
    "from scipy.linalg import toeplitz\n",
    "from scipy.linalg import solve\n",
    "from scipy.signal import lfilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /Users/owalid/42/post_intership/total-perspective-vortex/files/S001/S001R03.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19999  =      0.000 ...   124.994 secs...\n",
      "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
      "Extracting EDF parameters from /Users/owalid/42/post_intership/total-perspective-vortex/files/S001/S001R13.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19999  =      0.000 ...   124.994 secs...\n",
      "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
      "Extracting EDF parameters from /Users/owalid/42/post_intership/total-perspective-vortex/files/S001/S001R09.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19999  =      0.000 ...   124.994 secs...\n",
      "Used Annotations descriptions: ['T0', 'T1', 'T2']\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob('../files/S001/*.edf')\n",
    "'''\n",
    "=========  ===================================\n",
    "run        task\n",
    "=========  ===================================\n",
    "1          Baseline, eyes open\n",
    "2          Baseline, eyes closed\n",
    "3, 7, 11   Motor execution: left vs right hand\n",
    "4, 8, 12   Motor imagery: left vs right hand\n",
    "5, 9, 13   Motor execution: hands vs feet\n",
    "6, 10, 14  Motor imagery: hands vs feet\n",
    "=========  ===================================\n",
    "'''\n",
    "raws = []\n",
    "\n",
    "for i in [5, 9, 13]:\n",
    "    current_file = files[i]\n",
    "    r = read_raw_edf(current_file, preload=True, stim_channel='auto')\n",
    "    events, _ = mne.events_from_annotations(r)\n",
    "    if i in [5, 9, 13]:\n",
    "        new_labels_events = {1:'rest', 2:'action_hand', 3:'action_feet'} # action\n",
    "    new_annot = mne.annotations_from_events(events=events, event_desc=new_labels_events, sfreq=r.info['sfreq'], orig_time=r.info['meas_date'])\n",
    "    r.set_annotations(new_annot)\n",
    "    raws.append(r)\n",
    "    \n",
    "raw_obj = concatenate_raws(raws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (3,990) into shape (990,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/s9/mx70pkk11bq2wl2cnrt6nfwc0000gn/T/ipykernel_27220/351021454.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfreq_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# Estimate the MVAR model for this frequency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mmvar_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_mvar_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# Calculate the DTF for this frequency using the MVAR coefficients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/s9/mx70pkk11bq2wl2cnrt6nfwc0000gn/T/ipykernel_27220/351021454.py\u001b[0m in \u001b[0;36mestimate_mvar_model\u001b[0;34m(data, freq, model_order)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mlag_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmodel_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_channels\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmodel_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_order\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mlag_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlag\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_channels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlag\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_channels\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_samples\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmodel_order\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Define the target matrix for the MVAR model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (3,990) into shape (990,3)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from scipy.linalg import inv\n",
    "from scipy.signal import welch\n",
    "from scipy.linalg import solve_discrete_lyapunov\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def estimate_mvar_model(data, freq, model_order):\n",
    "    \"\"\"\n",
    "    Estimate an MVAR model for a specific frequency.\n",
    "\n",
    "    Parameters:\n",
    "    - data: Multivariate time series data (n_channels x n_samples).\n",
    "    - freq: Frequency of interest.\n",
    "    - model_order: Order of the MVAR model.\n",
    "\n",
    "    Returns:\n",
    "    - mvar_model: Estimated MVAR model.\n",
    "    \"\"\"\n",
    "    n_channels, n_samples = data.shape\n",
    "\n",
    "    # Define the lag matrix for the MVAR model\n",
    "    lag_matrix = np.zeros((n_samples - model_order, n_channels * model_order))\n",
    "    for lag in range(1, model_order + 1):\n",
    "        lag_matrix[:, (lag - 1) * n_channels:lag * n_channels] = data[:, lag:n_samples - model_order + lag]\n",
    "\n",
    "    # Define the target matrix for the MVAR model\n",
    "    target_matrix = data[:, model_order:]\n",
    "\n",
    "    # Fit the MVAR model\n",
    "    mvar_model = sm.OLS(target_matrix.T, lag_matrix).fit()\n",
    "\n",
    "    return mvar_model\n",
    "\n",
    "\n",
    "# Load your EEG/MEG data (replace with your data loading code)\n",
    "# Make sure your data is in the shape (n_channels, n_samples)\n",
    "\n",
    "# Define the model order for MVAR modeling\n",
    "model_order = 10\n",
    "\n",
    "# Define the sampling rate of your data (replace with the actual value)\n",
    "sample_rate = 1000  # Replace with your data's sampling rate in Hz\n",
    "\n",
    "# Define the frequency range of interest\n",
    "fmin = 1\n",
    "fmax = 30\n",
    "\n",
    "# Compute the cross-spectral density matrix\n",
    "frequencies, Cxy = welch(data, fs=sample_rate, nperseg=n_samples)\n",
    "\n",
    "# Initialize the DTF matrix\n",
    "dtf_matrix = np.zeros((n_channels, n_channels, len(frequencies)))\n",
    "\n",
    "# Loop over frequencies of interest\n",
    "for freq_idx, freq in enumerate(frequencies):\n",
    "    # Estimate the MVAR model for this frequency\n",
    "    mvar_model = estimate_mvar_model(data, freq, model_order)\n",
    "\n",
    "    # Calculate the DTF for this frequency using the MVAR coefficients\n",
    "    dtf_matrix[:, :, freq_idx] = calculate_dtf(mvar_model)\n",
    "\n",
    "# dtf_matrix contains the DTF values for each frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = raw_obj.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spectral_connectivity' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/s9/mx70pkk11bq2wl2cnrt6nfwc0000gn/T/ipykernel_27220/3033563635.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Compute PDC using spectral_connectivity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m pdc = spectral_connectivity(\n\u001b[0m\u001b[1;32m      6\u001b[0m     raw, method='pdc', mode='multitaper', fmin=fmin, fmax=fmax, faverage=True)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spectral_connectivity' is not defined"
     ]
    }
   ],
   "source": [
    "fmin = 1  # Minimum frequency\n",
    "fmax = 30  # Maximum frequency\n",
    "\n",
    "# Compute PDC using spectral_connectivity\n",
    "pdc = spectral_connectivity(\n",
    "    raw, method='pdc', mode='multitaper', fmin=fmin, fmax=fmax, faverage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "PNN\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Sample data (replace with your own dataset)\n",
    "# X_train, y_train = your_training_data\n",
    "# X_test, y_test = your_testing_data\n",
    "\n",
    "# Create a pipeline for PNN using Scikit-learn\n",
    "pnn = make_pipeline(\n",
    "    RBFSampler(gamma=1, random_state=1),\n",
    "    KNeighborsClassifier(n_neighbors=5)\n",
    ")\n",
    "\n",
    "# Train the PNN on your training data\n",
    "pnn.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the trained PNN\n",
    "y_pred = pnn.predict(X_test)\n",
    "\n",
    "# Evaluate the PNN\n",
    "accuracy = (y_pred == y_test).mean()\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PNN 2\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "class ProbabilisticNeuralNetwork(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, kernel='gaussian', bandwidth=1.0):\n",
    "        self.kernel = kernel\n",
    "        self.bandwidth = bandwidth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.classifiers_ = {}\n",
    "\n",
    "        for c in self.classes_:\n",
    "            X_class = X[y == c]\n",
    "            self.classifiers_[c] = KernelDensity(kernel=self.kernel, bandwidth=self.bandwidth)\n",
    "            self.classifiers_[c].fit(X_class)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        probabilities = np.zeros((X.shape[0], len(self.classes_)))\n",
    "\n",
    "        for i, c in enumerate(self.classes_):\n",
    "            log_density = self.classifiers_[c].score_samples(X)\n",
    "            probabilities[:, i] = np.exp(log_density)\n",
    "\n",
    "        return probabilities / probabilities.sum(axis=1, keepdims=True)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
    "\n",
    "# Sample data (replace with your own dataset)\n",
    "# X_train, y_train = your_training_data\n",
    "# X_test, y_test = your_testing_data\n",
    "\n",
    "# Create and train the PNN\n",
    "pnn = ProbabilisticNeuralNetwork(kernel='gaussian', bandwidth=0.1)\n",
    "pnn.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities and class labels\n",
    "probs = pnn.predict_proba(X_test)\n",
    "predicted_labels = pnn.predict(X_test)\n",
    "\n",
    "# Print predicted probabilities and labels\n",
    "print(\"Predicted Probabilities:\")\n",
    "print(probs)\n",
    "print(\"Predicted Labels:\")\n",
    "print(predicted_labels)\n",
    "\n",
    "\n",
    "\n",
    "PNN Keras\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class ProbabilisticLayer(Layer):\n",
    "    def __init__(self, num_units, smoothing_parameter=1.0, **kwargs):\n",
    "        super(ProbabilisticLayer, self).__init__(**kwargs)\n",
    "        self.num_units = num_units\n",
    "        self.smoothing_parameter = smoothing_parameter\n",
    "\n",
    "    def call(self, inputs):\n",
    "        pdfs = []\n",
    "        for i in range(self.num_units):\n",
    "            pdf = tf.exp(-0.5 * ((inputs - inputs[:, i:i+1]) / self.smoothing_parameter) ** 2)\n",
    "            pdf_sum = tf.reduce_sum(pdf, axis=1)\n",
    "            pdfs.append(pdf_sum)\n",
    "        return tf.stack(pdfs, axis=-1)\n",
    "\n",
    "# Sample data (replace with your own dataset)\n",
    "# X_train, y_train = your_training_data\n",
    "# X_test = your_testing_data\n",
    "\n",
    "# Create a PNN-like model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "    ProbabilisticLayer(num_units=X_train.shape[1]),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=-1)),  # Sum over the PDFs\n",
    "    keras.layers.Dense(np.max(y_train) + 1, activation='softmax')  # Output layer for classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = model.evaluate(X_test, y_test)[1]\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
